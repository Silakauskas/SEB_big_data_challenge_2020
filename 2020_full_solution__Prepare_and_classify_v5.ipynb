{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data\n",
      "Converting to dummies\n",
      "Save full and test IDs\n",
      "Remove columns\n",
      "Dealing with NAs\n",
      "Forming label y vector\n",
      "Pre-combined data shape: (9559809, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silaks\\AppData\\Local\\Temp\\ipykernel_2944\\2578939567.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ohe_data_combined_imputed_income = ohe_data_combined_imputed_income.append(ohe_test_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-combined data shape: (9569809, 35)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "#from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import preprocessing\n",
    "import calendar\n",
    "import time\n",
    "\n",
    "print(\"Importing data\")\n",
    "#test_data = pd.read_csv(\"C:/Users/silaks/Desktop/Simo/SEB Big Data Challange/Python/test_data_1.csv\")\n",
    "#data = pd.read_csv(\"C:/Users/silaks/Desktop/Simo/SEB Big Data Challange/Python/full.csv\")\n",
    "data = pd.read_csv(\"C:/Users/silaks/OneDrive - dematic.com/Desktop/python/full.csv\")\n",
    "local_test_data = pd.read_csv(\"C:/Users/silaks/OneDrive - dematic.com/Desktop/python/test_data_1.csv\")\n",
    "\n",
    "print(\"Converting categoricals to dummies (one hot encoding)\")\n",
    "# There are around 20 unique categories\n",
    "# As data is not too big, we can afford to create additional columns\n",
    "# With more data and time it would make sense to combine some categories based on similiarity\n",
    "one_hot_encoded_data = pd.get_dummies(data)\n",
    "one_hot_encoded_test_data = pd.get_dummies(local_test_data)\n",
    "\n",
    "print(\"Save full and test IDs\")\n",
    "# This will be required later to distinguish local test and train sets\n",
    "one_hot_encoded_test_data[\"test_id\"] = one_hot_encoded_test_data[\"id\"]\n",
    "one_hot_encoded_data[\"test_id\"] = 0\n",
    "one_hot_encoded_data[\"full_id\"] = one_hot_encoded_data[\"id\"]\n",
    "one_hot_encoded_test_data[\"full_id\"] = 0\n",
    "ohe_data = one_hot_encoded_data\n",
    "ohe_test_data = one_hot_encoded_test_data\n",
    "\n",
    "print(\"Remove columns\")\n",
    "# These columns do not exist in the test set\n",
    "ohe_data = ohe_data.drop('loan_purpose_UFVCU1VC', axis = 1)\n",
    "ohe_data = ohe_data.drop('loan_purpose_renewable_energy', axis = 1)\n",
    "# Could just ignore these columns in model later\n",
    "ohe_data = ohe_data.drop('id', axis = 1)\n",
    "ohe_test_data = ohe_test_data.drop('id', axis = 1)\n",
    "## ALL Y VALUES OF TEST SET ARE MISSING - THESE ARE UNKNOWN\n",
    "ohe_test_data = ohe_test_data.drop('y', axis = 1)\n",
    "## ALL Y VALUES OF TEST SET ARE MISSING - THESE ARE UNKNOWN\n",
    "\n",
    "print(\"Dealing with NAs\")\n",
    "###############################################################\n",
    "######### Remove NA rows\n",
    "ohe_data.drop(ohe_data[ohe_data[\"max_open_credit\"].isnull()].index, inplace = True) \n",
    "ohe_data.drop(ohe_data[ohe_data[\"bankruptcies\"].isnull()].index, inplace = True) \n",
    "ohe_data.drop(ohe_data[ohe_data[\"years_current_job\"].isnull()].index, inplace = True) \n",
    "###############################################################\n",
    "############ Replace NAs\n",
    "# If there was no delinquent ever - a highest value is placed\n",
    "# People who had delinquent very long time ago are more likely to be \"good\" loans\n",
    "ohe_data[\"months_since_last_delinquent\"] = ohe_data[\"months_since_last_delinquent\"].fillna(value = 300)\n",
    "ohe_test_data[\"months_since_last_delinquent\"] = ohe_test_data[\"months_since_last_delinquent\"].fillna(value = 300)\n",
    "# Alternative would be to introduce a categorical variable\n",
    "\n",
    "print(\"Forming label y vector\")\n",
    "labels = np.array(ohe_data['y'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "ohe_data = ohe_data.drop('y', axis = 1)\n",
    "\n",
    "\n",
    "#############################\n",
    "#### Try that gave good results with unknown test set - combining train and test data to impute missing values in both\n",
    "print(\"Pre-combined data shape:\",ohe_data.shape)\n",
    "ohe_data_combined_imputed_income = ohe_data\n",
    "ohe_data_combined_imputed_income = ohe_data_combined_imputed_income.append(ohe_test_data)\n",
    "ohe_data_combined_imputed_income = ohe_data_combined_imputed_income.sample(frac = 1)\n",
    "ohe_data_combined_imputed_income = ohe_data_combined_imputed_income.reset_index()\n",
    "ohe_data_combined_imputed_income = ohe_data_combined_imputed_income.drop('index', axis = 1)\n",
    "print(\"Post-combined data shape:\",ohe_data_combined_imputed_income.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term\n",
      "2\n",
      "['short' 'long']\n",
      "\n",
      "credit_score\n",
      "4\n",
      "['very_good' 'good' nan 'fair']\n",
      "\n",
      "loan_purpose\n",
      "16\n",
      "['other' 'debt_consolidation' 'home_improvements' 'take_a_trip'\n",
      " 'medical_bills' 'educational_expenses' 'major_purchase' 'small_business'\n",
      " 'business_loan' 'buy_a_car' 'moving' 'buy_house' 'vacation' 'wedding'\n",
      " 'renewable_energy' 'UFVCU1VC']\n",
      "\n",
      "home_ownership\n",
      "3\n",
      "['rent' 'mortgage' 'own']\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "####### IMPUTING YEARLY INCOME - imputed in train and test sets\n",
    "print(\"IMPUTING YEARLY INCOME\")\n",
    "imputer = KNNImputer(n_neighbors=2000)\n",
    "n = 25000  #chunk row size\n",
    "list_df = [ohe_data_combined_imputed_income[i:i+n] for i in range(0,ohe_data_combined_imputed_income.shape[0],n)]\n",
    "\n",
    "ts = time.gmtime()\n",
    "print(\"Imputing start time:\",time.strftime(\"%Y-%m-%d %H:%M:%S\", ts))\n",
    "    \n",
    "for i in range(1,len(list_df)):   \n",
    "    if i % 20 == 0:\n",
    "        ts = time.gmtime()\n",
    "        print(\"Imputing set (\",i*n,\" rows):\",i,\"   \",time.strftime(\"%Y-%m-%d %H:%M:%S\", ts))\n",
    "    list_df[i] = pd.DataFrame(imputer.fit_transform(list_df[i]),columns = list_df[i].columns)\n",
    "    \n",
    "ohe_data_full = pd.concat(list_df)\n",
    "ohe_data_full[\"yearly_income\"] = ohe_data_full[\"yearly_income\"].fillna(ohe_data_full[\"yearly_income\"].mean())\n",
    "\n",
    "\n",
    "########################################################\n",
    "####### LOAN AMOUNT 9999999\n",
    "# This has some sort of meaning - leaving it as a new flag column\n",
    "def loan_amount_category (row):\n",
    "    if row['amount_current_loan'] == 99999999 :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "ohe_data_full[\"loan_flag\"] = ohe_data_full.apply(lambda row: loan_amount_category(row), axis=1)\n",
    "ohe_data_full.loc[ohe_data_full.amount_current_loan == 99999999, 'amount_current_loan'] = None\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "####### IMPUTING LOAN AMOUNT\n",
    "# After setting the new flag - imputing the value to something realistic which will not mess the model\n",
    "imputer = KNNImputer(n_neighbors=2000)\n",
    "n = 25000  #chunk row size\n",
    "list_df = [ohe_data_full[i:i+n] for i in range(0,ohe_data_full.shape[0],n)]\n",
    "\n",
    "ts = time.gmtime()\n",
    "print(\"Imputing start time:\",time.strftime(\"%Y-%m-%d %H:%M:%S\", ts))\n",
    "    \n",
    "for i in range(1,len(list_df)):   \n",
    "    if i % 20 == 0:\n",
    "        ts = time.gmtime()\n",
    "        print(\"Imputing set (\",i*n,\" rows):\",i,\"   \",time.strftime(\"%Y-%m-%d %H:%M:%S\", ts))\n",
    "    list_df[i] = pd.DataFrame(imputer.fit_transform(list_df[i]),columns = list_df[i].columns)\n",
    "    \n",
    "ohe_data_full = pd.concat(list_df)\n",
    "ohe_data_full[\"amount_current_loan\"] = ohe_data_full[\"amount_current_loan\"].fillna(ohe_data_full[\"amount_current_loan\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ohe_data_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m########################################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m####### SEPARATING DATA FULL AND TEST (after imputation)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m ohe_test_data_fixed \u001b[38;5;241m=\u001b[39m \u001b[43mohe_data_full\u001b[49m[ohe_data_full[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m ohe_test_data_fixed \u001b[38;5;241m=\u001b[39m ohe_test_data_fixed\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m ohe_data_fixed \u001b[38;5;241m=\u001b[39m ohe_data_full[ohe_data_full[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ohe_data_full' is not defined"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "####### SEPARATING DATA FULL AND TEST (after imputation)\n",
    "ohe_test_data_fixed = ohe_data_full[ohe_data_full[\"test_id\"] != 0]\n",
    "ohe_test_data_fixed = ohe_test_data_fixed.sort_values(by=['test_id'])\n",
    "\n",
    "ohe_data_fixed = ohe_data_full[ohe_data_full[\"full_id\"] != 0]\n",
    "ohe_data_fixed = ohe_data_fixed.sort_values(by=['full_id'])\n",
    "\n",
    "ohe_data_fixed = ohe_data_fixed.drop('test_id', axis = 1)\n",
    "ohe_test_data_fixed = ohe_test_data_fixed.drop('test_id', axis = 1)\n",
    "ohe_data_fixed = ohe_data_fixed.drop('full_id', axis = 1)\n",
    "ohe_test_data_fixed = ohe_test_data_fixed.drop('full_id', axis = 1)\n",
    "\n",
    "\n",
    "########################################################\n",
    "####### RESETTING INDEXES\n",
    "ohe_data_scaled_imputed = ohe_data_fixed\n",
    "ohe_test_data_scaled_imputed = ohe_test_data_fixed\n",
    "ohe_test_data_scaled_imputed = ohe_test_data_scaled_imputed.reset_index()\n",
    "ohe_test_data_scaled_imputed = ohe_test_data_scaled_imputed.drop('index', axis = 1)\n",
    "ohe_data_scaled_imputed = ohe_data_scaled_imputed.reset_index()\n",
    "ohe_data_scaled_imputed = ohe_data_scaled_imputed.drop('index', axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "####### PREPARING FEATURES\n",
    "feature_list = list(ohe_data_scaled_imputed.columns)\n",
    "print(feature_list)\n",
    "\n",
    "#print(ohe_data.isna().sum())\n",
    "features = ohe_data_scaled_imputed\n",
    "#features = features.fillna(value = -1)\n",
    "features = np.array(features)\n",
    "\n",
    "features_final_test = ohe_test_data_scaled_imputed\n",
    "#features_final_test = features_final_test.fillna(value = -1)\n",
    "features_final_test = np.array(features_final_test)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "#train_features = features\n",
    "#train_labels = labels\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "print('FINAL Testing Features Shape:', features_final_test.shape)\n",
    "\n",
    "\n",
    "###----------- THIS PART BELOW WAS REPEATED A MILLION TIMES, CHANGING PARAMETERS BASED ON RESULTS -------------------\n",
    "\n",
    "print(\"Model train start time:\",time.strftime(\"%Y-%m-%d %H:%M:%S\", ts))\n",
    "rf1 = RandomForestClassifier(n_estimators = 300, \n",
    "                            max_depth = 36, \n",
    "                            min_samples_split = 800,\n",
    "                            n_jobs = -1,\n",
    "                            random_state = 42,\n",
    "                            max_features = 32,\n",
    "                            oob_score = True,\n",
    "                            class_weight = {0:4,1:1},\n",
    "                            min_samples_leaf = 300, \n",
    "                            verbose = 1)\n",
    "\n",
    "# Train the model on training data\n",
    "rf1.fit(train_features, train_labels)\n",
    "print(\"[28] AUC:\",metrics.roc_auc_score(test_labels, rf1.predict_proba(test_features)[:,1]))\n",
    "y_pred = rf1.predict_proba(features_final_test)[:,1]\n",
    "pd.DataFrame(y_pred).to_csv(\"test_results_28_prob_prediction.csv\")\n",
    "#24 - 0.79...\n",
    "#25 - 0.871377\n",
    "#26 - 0.8384 - 50 trees\n",
    "#27 - 0.8390 - 100 trees\n",
    "print(\"Model end time:\",time.strftime(\"%Y-%m-%d %H:%M:%S\", ts))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Model train start time:\",time.strftime(\"%Y-%m-%d %H:%M:%S\", ts))\n",
    "rf2 = RandomForestClassifier(n_estimators = 400, \n",
    "                            max_depth = 40, \n",
    "                            min_samples_split = 1000,\n",
    "                            n_jobs = -1,\n",
    "                            random_state = 42,\n",
    "                            max_features = 34,\n",
    "                            oob_score = True,\n",
    "                            class_weight = {0:4,1:1},\n",
    "                            min_samples_leaf = 500, \n",
    "                            verbose = 1)\n",
    "\n",
    "# Train the model on training data\n",
    "rf2.fit(train_features, train_labels)\n",
    "print(\"[29] AUC:\",metrics.roc_auc_score(test_labels, rf2.predict_proba(test_features)[:,1]))\n",
    "y_pred = rf2.predict_proba(features_final_test)[:,1]\n",
    "pd.DataFrame(y_pred).to_csv(\"test_results_29_prob_prediction.csv\")\n",
    "print(\"Model end time:\",time.strftime(\"%Y-%m-%d %H:%M:%S\", ts))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Model train start time:\",time.strftime(\"%Y-%m-%d %H:%M:%S\", ts))\n",
    "rf3 = RandomForestClassifier(n_estimators = 400, \n",
    "                            max_depth = 46, \n",
    "                            min_samples_split = 3000,\n",
    "                            n_jobs = -1,\n",
    "                            random_state = 42,\n",
    "                            max_features = 34,\n",
    "                            oob_score = True,\n",
    "                            class_weight = {0:3,1:1},\n",
    "                            min_samples_leaf = 1000, \n",
    "                            verbose = 1)\n",
    "\n",
    "# Train the model on training data\n",
    "rf3.fit(train_features, train_labels)\n",
    "print(\"[30] AUC:\",metrics.roc_auc_score(test_labels, rf3.predict_proba(test_features)[:,1]))\n",
    "y_pred = rf3.predict_proba(features_final_test)[:,1]\n",
    "pd.DataFrame(y_pred).to_csv(\"test_results_30_prob_prediction.csv\")\n",
    "print(\"Model end time:\",time.strftime(\"%Y-%m-%d %H:%M:%S\", ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "####### FEATURE IMPORTANCE\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(train_features.shape[1]):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, feature_list[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(train_features.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(train_features.shape[1]), indices)\n",
    "plt.xlim([-1, train_features.shape[1]])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
